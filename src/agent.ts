import type { ToolCall } from 'openai/resources/beta/threads/runs/steps.mjs'
import type { AIMessage } from '../types'
import { runLLM } from './llm'
import { addMessage, getMessages } from './memory'
import { logMessage, showLoader } from './ui'
import { runTool, saveToolResponse } from './toolRunner'
//Created a runAgent function that takes a userMessage(Through destructuring)
export const runAgent = async ({
  userMessage,
  tools,
}: {
  userMessage: string
  tools: any[]
}) => {
  // Adds the message to the array in db.json
  await addMessage([{ role: 'user', content: userMessage }])

  // Improrted showLoader from UI to display the thinking phase
  const loader = showLoader('Thinking...')

  // Gets all message history without its metadata (returns content and role)
  const history = await getMessages()

  // Call runLLm and pass the history and tools
  const response = await runLLM({ messages: history, tools })
  //(also pass the tools in runLLM and index.ts and runAgent)

  // Add the response of the LLM to the DB

  await addMessage([response])

  /**
   * Run an if statement which takes the tool_calls generated by response.
   * Update the loader to geenrate the tool call function name
   * Run runTool which takes @param {toolCall , userMessage} and save in tool response
   * execute saveToolResponse which takes @param {toolCall.id, toolReponse}
   * update the loader with the function name.
   *
   */

  if (response.tool_calls) {
    const toolCall = response.tool_calls[0]
    console.log('const toolCall = response.tool_calls[0] = ', toolCall)
    loader.update(toolCall.function.name)

    const toolResponse = await runTool(toolCall, userMessage)

    await saveToolResponse(toolCall.id, toolResponse)
  }

  // Log the message
  logMessage(response)

  //stop the loader from showing

  loader.stop()

  return getMessages()
}
